{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e56511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "import sounddevice as sd\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da90fe-fff0-488c-bad6-f46d5aec22db",
   "metadata": {},
   "source": [
    "### Create a matrix for audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471e1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "823e8099",
   "metadata": {},
   "source": [
    "### Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ee829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05badb60-4de2-4352-be6f-46043aa92c76",
   "metadata": {},
   "source": [
    "### Plot spectrograms some file and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd340651-a67b-4004-975d-e5c42b80008c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "261606bc-d1cc-4c6f-bc84-1770ef417558",
   "metadata": {},
   "source": [
    "### Plot MFCC some file and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb295f7a-7a44-437b-8cb9-98d6ed335a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataSet(Dataset):\n",
    "    def __init__(self, file_paths, labels, device, sr=44100,n_fft=1024,hop_length=512, n_mfcc=13,n_mels=128, duration=5):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.sampled_sr = sr\n",
    "        self.n_fft = n_fft\n",
    "        self.duration = duration\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mfcc= n_mfcc\n",
    "        self.device = device\n",
    "        self.n_mels = n_mels\n",
    "\n",
    "        self.melspectrogram_dbs= []\n",
    "        for index in range(len(file_paths)):\n",
    "            mel_spec_db = self.__get_melspectrogram_db__(index)\n",
    "            mel_spec_db = torch.tensor(mel_spec_db, dtype=torch.float32).unsqueeze(0)\n",
    "            self.melspectrogram_dbs.append(mel_spec_db)\n",
    "\n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.melspectrogram_dbs[index], self.labels[index]\n",
    "    \n",
    "    def __load_audio__(self, index):\n",
    "        audio_sample_path = self.file_paths[index]\n",
    "        signal, sr = librosa.load(audio_sample_path, sr=None)\n",
    "        y, x = torchaudio.load(audio_sample_path)\n",
    "        # Get the first 5s (duration) of the audio\n",
    "        signal = self.__get_audio_duration__(signal, sr)\n",
    "        signal, sr = self.__resample_audio__(signal, sr)\n",
    "        return signal, sr\n",
    "\n",
    "    def __get_audio_duration__(self, signal,sr):\n",
    "        if signal.shape[0]<self.duration*sr:\n",
    "            signal=np.pad(signal,int(np.ceil((self.duration*sr-signal.shape[0])/2)),mode='reflect')\n",
    "        else:\n",
    "            signal=signal[:self.duration*sr]\n",
    "        \n",
    "        return signal\n",
    "\n",
    "    def __resample_audio__(self, signal, sr):\n",
    "        if sr != self.sampled_sr:\n",
    "            signal_resampled = librosa.resample(signal, orig_sr=sr, target_sr=self.sampled_sr)\n",
    "            return signal_resampled, self.sampled_sr\n",
    "        return signal, sr\n",
    "\n",
    "    def __get_melspectrogram_db__(self, index):\n",
    "        signal, sr = self.__load_audio__(index)\n",
    "        ms = librosa.feature.melspectrogram(y=signal, sr=sr,fmax=sr// 2,n_mels=self.n_mels)\n",
    "        mel_spec_db = librosa.power_to_db(ms, ref=np.max)\n",
    "        return self.__melspec_normalization__(mel_spec_db)\n",
    "\n",
    "    def __melspec_normalization__(self,mel_spec_db):\n",
    "        # Normalize to [0, 1]\n",
    "        return (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())\n",
    "\n",
    "    def __plot_mfcc__(self, index):\n",
    "        signal, sr = self.__load_audio__(index)\n",
    "        hop_size = self.hop_length//2\n",
    "        mfccs_librosa = librosa.feature.mfcc(y=signal, sr=sr, n_fft=self.n_fft, hop_length=hop_size)\n",
    "        plt.figure()\n",
    "        plt.imshow(mfccs_librosa, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.colorbar(format='%+2.0f')\n",
    "        plt.title(\"MFCCs\")\n",
    "        plt.xlabel(\"Time Frames\")\n",
    "        plt.ylabel(\"MFCC Coefficients\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def __plot_spectrogram__(self, index):\n",
    "        signal, sr = self.__load_audio__(index)\n",
    "        nfft = self.n_fft\n",
    "        win_size = nfft\n",
    "        hop_size = nfft//2\n",
    "        librosa_spectrogram = librosa.stft(signal,n_fft=nfft, hop_length=hop_size, win_length=win_size)\n",
    "        librosa_power_spectrogram = librosa.amplitude_to_db(librosa_spectrogram, ref=np.max)\n",
    "\n",
    "        plt.figure()\n",
    "        librosa.display.specshow(librosa_power_spectrogram, sr=sr, x_axis='time', y_axis='hz', cmap='viridis')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Spectrogram')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Frequency (Hz)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def __play_audio__(self, index):\n",
    "        signal, sr = self.__load_audio__(index)\n",
    "        plt.figure()\n",
    "        plt.plot(signal)\n",
    "        plt.show()\n",
    "        sd.play(signal)\n",
    "        sd.wait()\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf06a7-361e-41f6-81ef-f13ff72bfa51",
   "metadata": {},
   "source": [
    "### Implement the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da48473e-66eb-4fa7-ad92-80cdf70e919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_loss_and_optimizer(model:nn.Module, learning_rate:float=0.001):\n",
    "    \"\"\"\n",
    "    :param model:nn.Module\n",
    "    :param learning_rate:float = 0.001\n",
    "    :return: loss_function:nn.Module, optimizer:nn.Module\n",
    "\n",
    "    Return Cross Entropy Loss function and Adam optimizer\n",
    "    \"\"\"\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return [loss_function, optimizer]\n",
    "\n",
    "def get_total_false(label):\n",
    "    false_count = 0\n",
    "\n",
    "    for index in range(len(label)):\n",
    "\n",
    "        if label[index] == 0:\n",
    "            false_count += 1\n",
    "    return false_count\n",
    "\n",
    "def get_total_true(label):\n",
    "    true_count = 0\n",
    "\n",
    "    for index in range(len(label)):\n",
    "\n",
    "        if label[index] == 0:\n",
    "            true_count += 1\n",
    "    return true_count\n",
    "\n",
    "def custom_loss(data_label, pred_label):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    for index in range(len(data_label)):\n",
    "        if pred_label[index] == data_label[index]:\n",
    "            true_positive += 1\n",
    "        elif pred_label[index] == 1 & data_label[index] == 0:\n",
    "            false_positive += 1\n",
    "        elif pred_label[index] == 0 & data_label[index] == 1:\n",
    "            false_negative += 1\n",
    "        else:\n",
    "            true_negative += 1\n",
    "        \n",
    "    precision = true_positive / (true_positive + true_negative)\n",
    "    recall =true_positive / (true_positive + false_positive)\n",
    "    \n",
    "    return precision, recall\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a2f1e-bd41-4b16-9e2b-c551899bdaee",
   "metadata": {},
   "source": [
    "### Train the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc6983-759e-489e-adff-6180b6d257cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNetwork(nn.Module):\n",
    "    #def __init__(self, time_frames, n_mels=128):  # Accepts time_frames and n_mels\n",
    "    def __init__(self, n_mels=128):  # Accepts time_frames and n_mels\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        #self.time_frames = time_frames\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)  # Reduces dimensions by half\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate the dimensions of the feature map after convolutional layers\n",
    "        final_height = self._calc_final_dim(self.n_mels, num_pooling_layers=4)\n",
    "        final_width = self._calc_final_dim(self.time_frames, num_pooling_layers=4)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        \n",
    "       \n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(in_features=128 * final_height * final_width, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)  # Regularization\n",
    "        )\n",
    "        \"\"\"\n",
    "        print(\"self.flatten shape\",self.flatten.__sizeof__())\n",
    "        self.linear2 = nn.Linear(in_features=128 *208, out_features=2)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output = nn.Softmax(dim=1)\n",
    "\n",
    "    def _calc_final_dim(self, input_dim, num_pooling_layers):\n",
    "        \"\"\"\n",
    "        Calculate the final dimension after convolution and pooling layers.\n",
    "        Each pooling layer halves the input dimension.\n",
    "        \"\"\"\n",
    "        for _ in range(num_pooling_layers):\n",
    "            input_dim = input_dim // 2\n",
    "        return input_dim\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)  # Flatten feature maps into a 1D vector\n",
    "        #x = self.linear1(x)  # Dense hidden layer\n",
    "        logits = self.linear2(x)  # Final linear layer\n",
    "        output = self.output(logits)  # Apply sigmoid for binary classification\n",
    "        return output\n",
    "\n",
    "#model=CNNNetwork().cuda()\n",
    "#summary(model,(1,128,430))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c9a4fd-cb22-45ad-8040-aa9187da256b",
   "metadata": {},
   "source": [
    "### Calculate results and analyze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b247ffb-b7eb-4d55-a9c1-d283bbcd7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "def getFilePaths(path):\n",
    "    return [path +'/'+ file for file in os.listdir(path)]\n",
    "\n",
    "def create_data_loader(train_data, batch_size):\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    return train_dataloader\n",
    "\n",
    "def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n",
    "    loss = None\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # calculate loss\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "\n",
    "        # backpropagate error and update weights\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n",
    "        print(\"---------------------------\")\n",
    "    print(\"Finished training\")\n",
    "\n",
    "\n",
    "\n",
    "audio_paths = []\n",
    "audio_labels = []\n",
    "categories = {\n",
    "    0: \"bus\",\n",
    "    1: \"tram\"\n",
    "}\n",
    "for key, value in categories.items():\n",
    "    paths = getFilePaths('dataset/' + value)\n",
    "    audio_paths += paths\n",
    "    audio_labels += [key]*len(paths)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device {device}\")\n",
    "audio_data = SoundDataSet(file_paths=audio_paths, labels=audio_labels, device=device)\n",
    "#audio_data.__plot_spectrogram__(26)\n",
    "#audio_data.__plot_mfcc__(26)\n",
    "#audio_data.__play_audio__(26)\n",
    "model = CNNNetwork()\n",
    "loss_func, optimizer = torch_loss_and_optimizer(model=model, learning_rate= LEARNING_RATE)\n",
    "train_data_loader = create_data_loader(audio_data, BATCH_SIZE)\n",
    "train(model, train_data_loader,loss_func, optimizer, device, EPOCHS)\n",
    "if False:\n",
    "    torch.save(model.state_dict(), \"vehicle_audio_processing_model.pth\")\n",
    "    print(\"Trained feed forward net saved at vehicle_audio_processing_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
